{
"Question1": "Please compare the similarities and differences between adaptive control and conventional feedback control. (Answer Correct)",
"Answer": "Both adaptive control and conventional feedback control are commonly used methods in control systems, aiming to achieve system stability and performance optimization. However, they have significant differences in design philosophy, application scenarios, and complexity. The following is a comparison of their similarities and differences:\n1.1 Similarities:\n    - Feedback Mechanism: Both are based on the feedback principle. They adjust the control input by measuring the error between the system output and the expected value to reduce the error and maintain system stability.\n    - Consistent Objectives: Both strive to improve the system's robustness, tracking accuracy, and anti-disturbance capability, for example, in industrial process control or robot systems.\n    - Similar Structure: They usually include basic components such as sensors, controllers, and actuators, forming a closed-loop control structure.\n1.2 Differences:\n    - Parameter Adjustment Method:\n      Conventional feedback control (e.g., PID control) uses fixed controller parameters. The design is based on a known or approximate system model, and the parameters do not adjust with system changes.\n      Adaptive control can adjust controller parameters in real-time online to cope with dynamic system changes or uncertainties, for example, through parameter estimation or model reference mechanisms.\n    - Ability to Handle Uncertainties:\n      Conventional feedback control performs well when the system model is accurate and disturbances are small, but it is sensitive to parameter changes or unmodeled dynamics, which may lead to performance degradation.\n      Adaptive control is specifically designed for system uncertainties (such as unknown parameters, time-variation, or external disturbances). It automatically compensates for these factors through adaptive laws, improving the system's robustness in unknown environments.\n    - Complexity and Design:\n      Conventional feedback control has a simple design, is easy to implement and maintain, and is suitable for systems with fixed models and little change (e.g., temperature-controlled incubators).\n      Adaptive control has a more complex design, involving online estimation, stability analysis (e.g., Lyapunov method), and computing resources. It is suitable for dynamically changing scenarios such as aerospace or automotive control.\n    - Examples:\n      Conventional feedback control: The PID controller in household air conditioners, with fixed parameters, adjusts the output according to the set temperature.\n      Adaptive control: Aircraft autopilot systems, which use adaptive control to compensate for changes in aerodynamic parameters and ensure flight stability.\n\nSummary: Conventional feedback control is suitable for systems with known and stable models, while adaptive control is more flexible and can handle uncertain and time-varying systems, but it has higher design complexity.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}

{
"Question2": "How to understand the statement about 'uncertainty of the controlled object' in adaptive control? (Answer Correct)",
"Answer": "In adaptive control, the 'uncertainty of the controlled object' refers to the existence of unknown, variable, or unpredictable factors in the system model or its parameters, which may affect control performance. The following is an explanation from the perspectives of concept, types, and adaptive mechanisms:\n2.1 Concept Explanation:\nThe uncertainty of the controlled object means that the mathematical model of the system is incomplete or inaccurate, such as unknown or time-varying parameter values, or the presence of unmodeled dynamics and external disturbances. Adaptive control 'learns' and compensates for these uncertainties by adjusting the controller online, thereby maintaining the desired performance.\nFor example, in robotic arm control, the load mass may change, leading to uncertain dynamic parameters; adaptive control can estimate these parameters in real time and adjust the torque output.\n2.2 Types of Uncertainties:\n    - Parameter Uncertainty: System parameters (such as mass, damping coefficient) are unknown or time-varying. For instance, the reaction rate in chemical processes changes with temperature.\n    - Structural Uncertainty: The system model itself is inaccurate, such as ignoring high-order dynamics or nonlinear effects.\n    - External Disturbances: Unmeasurable environmental interferences, such as the impact of wind disturbance on unmanned aerial vehicles (UAVs).\nAdaptive control addresses these types through the design of adaptive laws (such as parameter estimators or reference model tracking) to ensure the system operates stably under uncertainty.\n2.3 Understanding of Adaptive Mechanisms:\nAdaptive controllers continuously monitor system outputs and performance indicators, use algorithms (such as gradient methods or least squares methods) to estimate uncertainties online, and adjust controller parameters. This is similar to a 'self-optimization' process, enabling the system to maintain robustness in unknown environments.\nExample: In automotive cruise control, changes in road gradients lead to uncertainties in vehicle dynamics; adaptive control compensates by adjusting the throttle to maintain a constant speed.\nSummary: Adaptive control provides a dynamic compensation mechanism for the uncertainty of the controlled object, and improves the system's adaptability in unknown or changing environments through real-time adjustments.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}

{
"Question3": "What is the definition of adaptive control?",
"Answer": "Adaptive control is an advanced control strategy whose core lies in the controller's ability to automatically adjust its own parameters to cope with dynamic system changes or uncertainties. The following is its systematic definition:\n3.1 Basic Definition:\n    Adaptive control is a nonlinear control method in which controller parameters are adjusted in real time based on online measured system performance data to maintain or optimize the behavior of the closed-loop system, despite the presence of model uncertainties, time-varying parameters, or external disturbances.\n    It combines feedback control with an online learning mechanism, enabling the system to have a 'self-adaptive' capability.\n3.2 Key Features:\n    Online Parameter Adjustment: Controller parameters are not fixed but dynamically updated through adaptive laws (such as estimation or optimization algorithms).\n    Performance Monitoring: Continuously evaluates system outputs (such as tracking errors or stability indicators) and uses this to drive parameter adjustments.\n    Handling Uncertainties: Specifically designed to handle unknown or time-varying system parameters and improve robustness.\n    Example: In power systems, adaptive control is used to adjust generator excitation to cope with load changes and network impedance uncertainties.\n3.3 Mathematical Expression:\n    Typically, an adaptive control system consists of two loops: the inner loop is conventional feedback control, and the outer loop is a parameter adjustment loop. For example, in Model Reference Adaptive Control (MRAC), the system output is forced to track the output of an ideal reference model.\n    The theoretical basis of adaptive control often involves Lyapunov stability theory to ensure parameter convergence and system stability.\n\nSummary: By adjusting controller parameters in real time, adaptive control enables the system to automatically maintain performance in an uncertain environment, and it is an important branch of intelligent control.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}
{
"Question4": "How is adaptive control classified?",
"Answer": "Adaptive control can be classified based on design methods, structures, and application scenarios. The following is a systematic classification method based on common standards:\n4.1 Classification Based on Design Methods:\n    Model Reference Adaptive Control (MRAC):\n    A reference model is used to define the desired system performance, and controller parameters are adjusted to make the output of the controlled system track the output of the reference model. It is suitable for scenarios requiring precise tracking, such as aerospace and robotic systems.\n    Example: In aircraft attitude control, the reference model represents the ideal flight path, and the adaptive law adjusts the control surfaces to minimize the tracking error.\n    Self-Tuning Control (STC):\n    First, the parameters of the controlled object are estimated online, and then the controller parameters are designed or updated based on the estimated model. It usually includes a parameter estimator and a controller design module. It is suitable for process control and industrial automation.\n    Example: In humidity control during the papermaking process, STC estimates process parameters in real time and adjusts the heater output to maintain the target humidity.\n    Other Classifications:\n    Direct Adaptive Control: Adjusts controller parameters directly without explicit parameter estimation, such as some MRAC variants (the meaning is not clear, but MRAC does belong to direct adaptive control).\n    Indirect Adaptive Control: First estimates system parameters, then designs the controller indirectly, such as STC (STC is Self-Tuning Control, which includes direct algorithms and indirect algorithms).\n    Gain Scheduling Adaptive Control: Adjusts parameters based on a predefined schedule, suitable for systems with measurable parameter changes, such as the adjustment of aircraft control gains at different altitudes.\n4.2 Classification Based on Application Fields:\n    Continuous-Time Adaptive Control: Used for analog systems, such as mechanical and electrical systems.\n    Discrete-Time Adaptive Control: Used for digital control systems, such as computer-controlled industrial processes.\n(Continuous-time and discrete-time adaptive control mainly differ in terms of mathematical expression; currently, computer control is widely used, whether for mechanical systems, electrical systems, or industrial processes.)\n    Adaptive Types: Including parameter adaptation (to handle parameter uncertainties) and structural adaptation (to handle changes in model structure).\n\n  Selection Basis:\n    The choice of classification depends on system characteristics: MRAC is suitable for high-performance tracking, while STC is suitable for scenarios dominated by parameter estimation. In practical applications, multiple methods are often combined to balance complexity and performance.\n\nSummary: The classification of adaptive control is mainly based on the two major paradigms of MRAC and STC, and other variants expand its applicability, helping engineers select appropriate methods according to the type of uncertainty and system requirements.",
"Rate": 2,
"Category": "FDC",
"Comment": "The answer is basically correct."
}
{
"Question5": "Please compare the similarities and differences between Self-Tuning Control (STC) and Model Reference Adaptive Control (MRAC).",
"Answer": "Self-Tuning Control (STC) and Model Reference Adaptive Control (MRAC) are two mainstream methods of adaptive control. They have both similarities and significant differences in design philosophy, implementation mechanism, and application scenarios. The following is a detailed comparison:\n5.1 Similarities:\n    Adaptive Nature: Both can adjust controller parameters online to handle system uncertainties, such as unknown or time-varying parameters, thereby improving system robustness and performance.\n    Feedback Structure: Both are based on closed-loop control and include a parameter adjustment loop to ensure stable system operation.\n    Application Fields: Both are widely used in fields such as industrial automation, aerospace, and robotics. For example, in automotive engine control, both can be used to compensate for changes in fuel injection parameters.\n    Theoretical Foundation: Both share stability analysis tools (e.g., Lyapunov theory) (Note: The theoretical foundation of Self-Tuning Control is not Lyapunov theory) and need to address issues of parameter convergence and disturbance sensitivity.\n5.2 Differences:\n    Design Principle:\n    MRAC: Uses a predefined reference model to represent the desired system dynamics, and directly adjusts controller parameters by minimizing the error between the output of the controlled system and the output of the reference model. It emphasizes performance tracking.\n    STC: First estimates the parameters of the controlled object online (e.g., using the recursive least squares method), then designs or updates controller parameters (e.g., PID parameters) based on the estimated model. It emphasizes model identification and controller redesign.\n    Implementation Complexity:\n    MRAC is usually more direct, but it requires designing an appropriate reference model and adaptive law, and may be more sensitive to model errors.\n    STC involves two steps: parameter estimation and controller design, which has a larger computational load but is more flexible, and is suitable for systems with slow parameter changes.\n    Performance Characteristics:\n    MRAC has better tracking performance and can respond quickly to changes, but its stability analysis is more complex; for example, in robot trajectory tracking, MRAC can ensure accurate following of the reference path (Note: MRAC can only achieve asymptotic tracking, not accurate tracking).\n    STC focuses more on parameter convergence and model accuracy, and its response may be slower, but it is suitable for slow-varying systems in process control, such as temperature control of chemical reactors.\n    Examples:\n    MRAC: UAV flight control, where the reference model defines the ideal flight attitude, and the adaptive system adjusts the servo output to cope with wind disturbances.\n    STC: Steel rolling process, where STC estimates the stiffness parameters of the rolling mill in real time and adjusts the roll gap control to maintain the thickness of the steel plate.\n\nSummary: Both MRAC and STC are effective adaptive methods. MRAC focuses on direct performance tracking, while STC focuses on indirect parameter estimation and controller update. The selection depends on specific application requirements: MRAC is suitable for high-performance tracking scenarios, and STC is suitable for slow-varying systems dominated by parameter estimation.",
"Rate": 2,
"Category": "FDC",
"Comment": "The answer is basically correct."
}

{
"Question6": "What is the significance of researching the theoretical issues of adaptive control?",
"Answer": "Researching the theoretical issues of adaptive control is not only for designing the controller itself, but also for ensuring its reliability and effectiveness in the real world. Its core significance lies in the following aspects:\nGuaranteeing system stability: Adaptive systems are nonlinear and time-varying. Even if parameters converge, the system itself may become unstable. Theoretical analysis (such as Lyapunov stability theory) is the only way to ensure the global stability of the system during all learning processes.\nAnalyzing convergence: Theoretical research answers key questions: 'Can the controller parameters converge to the ideal values?' and 'Can the system output converge to the desired trajectory?' This directly determines the quality of control performance.\nEvaluating robustness: Actual systems always have unmodeled dynamics, measurement noise, and disturbances. Theoretical research can define the robustness of adaptive control, that is, the ability to maintain stability and performance under these non-ideal conditions.\nGuiding engineering applications: The design of adaptive control without theoretical guidance is blind. Theory provides engineers with design criteria and stability boundaries, avoiding catastrophic failures caused by empiricism in critical applications (such as aerospace and medical equipment).\nExample: Without proof from stability theory, an adaptive controller designed for an aircraft might generate divergent control commands when encountering sudden air currents (unmodeled dynamics), leading to loss of flight control.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}


{
"Question7": "Why is it necessary to first study the dynamic model of the controlled object when researching adaptive control?",
"Answer": "Studying the dynamic model of the controlled object is the cornerstone of designing any effective control system, and it is particularly important for adaptive control:\nDetermining the controller structure: The dynamic model reveals the basic physical laws of the system (such as inertia, damping, and energy conservation), which directly determines the basic structure and form of the controller (e.g., whether to use PID, state feedback, or other structures).\nDesigning the adaptive law: The core of adaptive control lies in 'how to adjust parameters'. The derivation of this adjustment rule (adaptive law) heavily relies on the mathematical form of the dynamic model. For example, the Lyapunov-based design method requires the error equation of the model.\nUnderstanding the scope of uncertainty: Although adaptive control deals with 'uncertainty', we need to know which part of the model the uncertainty exists in (e.g., which parameters are unknown and what the system order is). A rough model framework is a prerequisite for starting the adaptive process.\nGuaranteeing performance and stability: A deep understanding of the dynamic model helps predict system behavior and theoretically ensure that the designed adaptive controller can be stable and achieve the expected performance.\nExample: To design an adaptive control for a robotic arm, it is necessary to know in advance that its dynamic model has the form M(q)q'' + C(q, q')q' + G(q) = τ, even if the specific parameters in matrices M, C, and G are unknown. The adaptive law is designed based on this model structure to estimate these unknown parameters online.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}

{
"Question8": "What are the differences and connections between the two concepts of 'dynamic model' and 'parameter estimation'?",
"Answer": "Differences:\nDynamic model: A mathematical expression that describes the law of the system state evolving over time, usually a set of differential or difference equations. It defines the structure of the system, such as ẋ(t) = Ax(t) + Bu(t).\nParameter estimation: A process or algorithm used to determine the unknown or changing numerical values in the model from input-output data. For example, determining the specific element values of matrices A and B in the above model.\nConnections:\nMutual dependence: Parameter estimation must be carried out within the structural framework of a certain dynamic model; while an accurate dynamic model requires correct parameter values to have practical value.\nCollaboration in adaptive control: In indirect adaptive control (such as self-tuning control), there is first a dynamic model structure, then parameter estimation is used to identify model parameters online, and finally the controller is redesigned based on the updated model.\nSummary: The dynamic model is the 'skeleton' of the system, while parameter estimation is the 'flesh and blood' that fills the skeleton. The skeleton determines what to estimate, and the flesh and blood make the skeleton concrete and usable.",
"Rate": 1,
"Category": "FDC",
"Comment": "noen"
}

{
"Question9": "What is the difference between a dynamic model and a static model?",
"Answer": "The core difference between a dynamic model and a static model lies in whether the time factor and the change process of the system state are considered.\nTime dimension: The dynamic model clearly describes the dynamic process of the system state, input, and output changing over time, and its behavior is a function of time. In contrast, the static model completely ignores the time factor and only describes the fixed relationship between input and output after the system reaches a balanced state.\nMathematical description: Dynamic models are usually characterized by differential equations (for continuous time) or difference equations (for discrete time) to depict system dynamics. Static models are simply represented by algebraic equations.\nSystem behavior: Dynamic models can reveal the transient performance of the system, such as response speed, overshoot, and stability. Static models can only reflect the steady-state performance of the system, i.e., the final equilibrium point.\nCausal relationship: In a dynamic model, the current output depends not only on the current input but also on the past state of the system (initial conditions), and it has 'memory'. In a static model, the output is uniquely determined only by the input at the same moment.\nExample: Describing the water temperature change of a heating system requires the use of a dynamic model (such as a differential equation), which can tell us how long it takes for the water temperature to rise from 20°C to 80°C and the temperature curve during the process. However, simply stating that 'when the heating power is set to 1000W, the water temperature will eventually stabilize at 80°C' is a static model.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}

{
"Question10": "What is the difference between a deterministic model and a stochastic model?",
"Answer": "The fundamental difference between a deterministic model and a stochastic model lies in whether uncertainty is explicitly considered and expressed in the model.\nDeterminism of prediction: In a deterministic model, given the same initial conditions and inputs, the output and future state of the system are completely unique and predictable. In a stochastic model, even if the initial conditions and inputs are exactly the same, the output of the system may be different; its results are uncertain and can only be described by probability distributions.\nModel content: A deterministic model assumes that we have complete knowledge of the system, all parameters and relationships are accurately known, and no random factors are included. A stochastic model explicitly incorporates randomness into it, usually including random variables, noise terms, or probability distributions to characterize uncertainties such as measurement noise, environmental disturbances, or unmodeled dynamics.\nMathematical tools: Deterministic models usually use classical mathematical analysis tools, such as ordinary differential equations. Stochastic models require the use of probability theory and mathematical statistics tools, such as stochastic differential equations.\nApplication scenarios: Deterministic models are suitable for ideal environments or situations where the dominant dynamics of the system are very clear. Stochastic models are more close to the real world and are used to handle systems with significant random disturbances.\nExample: Calculating the motion trajectory of an object on a smooth plane according to Newton's second law F = ma is a deterministic model with a unique result. A weather forecast model that predicts tomorrow's rainfall is a stochastic model, because it needs to handle a large number of uncertain atmospheric variables, and its results are usually presented in the form of '60% probability of precipitation'. In control systems, a UAV model that includes sensor noise and wind disturbances is a stochastic model.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}

{
"Question11": "What is the difference between a discrete-time model and a continuous-time model?",
"Answer": "The core difference between discrete-time models and continuous-time models lies in how they describe the time variable and the system's evolutionary process over time.\nTime Domain:\nContinuous-time model: The time variable t takes continuous values (e.g., all real numbers). It describes the dynamic changes of the system state at every moment.\nDiscrete-time model: The time variable k takes values only at discrete time points (usually equally spaced integers, such as k=0, 1, 2, ...). It describes the behavior of the system state at these specific sampling moments.\nMathematical Tools:\nContinuous-time model: Usually described by differential equations. For example, the state-space model: ẋ(t) = Ax(t) + Bu(t).\nDiscrete-time model: Usually described by difference equations. For example, the state-space model: x(k+1) = A_d x(k) + B_d u(k).\nApplication Scenarios:\nContinuous-time model: Typically derived from first-principle analysis of the physical world (such as mechanical and electrical systems) and serves as the descriptive language of natural sciences.\nDiscrete-time model: A direct tool for the design and analysis of digital control systems (such as controllers implemented by computers and microprocessors), because digital computers operate discretely according to clock cycles.\nExample: The charging process of an RC circuit, where the change in capacitor voltage is described by the differential equation dVc(t)/dt = (Vin - Vc(t)) / (RC) — this is a continuous-time model. If we use a microcontroller to measure the voltage once per second and adjust the input accordingly, the model used for controller design is a discrete-time model, which may take the form Vc(k+1) = a * Vc(k) + b * Vin(k).",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}

{
"Question12": "What is the difference between an input-output model and a state-space model?",
"Answer": "The main difference between input-output models and state-space models lies in the degree and manner in which they describe the internal information of a system.\n1. Description Perspective:\n   - Input-output model (external model): Only describes the relationship between the system's external input variable u(t) and output variable y(t), without revealing the system's internal structure. It is like a 'black box'.\n   - State-space model (internal model): Not only describes the input-output relationship, but also reveals the system's internal behavior and energy storage by introducing state variables x(t). It is like a 'white box'.\n2. Mathematical Expression:\n   - Input-output model: Usually takes the form of a high-order differential/difference equation or a transfer function. For example: y''(t) + a₁y'(t) + a₀y(t) = b₀u(t).\n   - State-space model: Consists of a set of interrelated first-order differential/difference equations, including the state equation and the output equation:\n     ẋ(t) = Ax(t) + Bu(t)\n     y(t) = Cx(t) + Du(t)\n3. Information Quantity and Design Applicability:\n   - Input-output model: Sufficient for designing simple controllers (such as PID), but it is difficult to apply to optimal control (such as LQR) and state observer design.\n   - State-space model: Provides more comprehensive system information, facilitating the design of multivariable control and optimal control, and enabling the handling of initial conditions. All internal variables (states) can be used for feedback to implement more advanced control strategies.\n4. Example:\n   For a mass-spring-damper system, the input-output model only focuses on the relationship between the external force (input) and the displacement of the mass block (output). In contrast, the state-space model defines both displacement and velocity as state variables, thereby fully describing the system's kinetic energy and potential energy.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}

{
"Question13": "What is the difference between a linear model and a nonlinear model?",
"Answer": "The fundamental difference between linear models and nonlinear models lies in whether they satisfy the superposition principle.\n1. Superposition Principle:\n   - Linear model: Satisfies the superposition principle. Specifically, if input u₁ produces output y₁ and input u₂ produces output y₂, then input a*u₁ + b*u₂ will produce output a*y₁ + b*y₂ (where a and b are constants).\n   - Nonlinear model: Does not satisfy the superposition principle. The system's response to a combination of multiple inputs cannot be simply obtained by superimposing the responses of each input acting alone.\n2. Mathematical Expression and Properties:\n   - Linear model: Variables and their derivatives in the equation only appear in the form of first power, with no cross-multiplication terms. It has rich mathematical tools (such as Laplace transform and eigenvalue analysis), and there are general methods for analysis and design.\n   - Nonlinear model: Contains nonlinear terms such as high powers of variables, trigonometric functions, and products. There is no universal method for analysis and synthesis; its behavior is complex and may exhibit characteristics that linear systems do not have, such as limit cycles and chaos.\n3. Scope of Application:\n   - Linear model: Usually an approximate model obtained by linearizing a nonlinear system near the equilibrium point, and is only valid within a small range.\n   - Nonlinear model: Can more accurately describe the real dynamics of most actual physical systems within the full operating range.\n4. Example:\n   A pendulum swinging at a small angle has a linear model: θ''(t) + (g/L)θ(t) = 0. A pendulum swinging at a large angle has an accurate nonlinear model: θ''(t) + (g/L)sin(θ(t)) = 0, because it contains the nonlinear term sin(θ).",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}

{
"Question14": "How to describe a discrete-time linear system using polynomials?",
"Answer": "The most common way to describe a discrete-time linear system using polynomials is through difference equations and their corresponding pulse transfer functions.\n1. Difference Equation Form:\n   The difference equation of a single-input single-output (SISO) system is usually written as:\n   y(k) + a₁y(k-1) + ... + a_ny(k-n) = b₀u(k) + b₁u(k-1) + ... + b_mu(k-m)\n   This equation describes the linear combination relationship between the current output y(k), the previous n outputs, and the current and previous m inputs u(k). (Note: Continuous-time systems may have no time delay, but when converted to discrete-time systems, at least one-sample time delay will naturally occur. That is, y(k) can only correspond to u(k-1) and previous values, not u(k).)\n2. Pulse Transfer Function Form (Polynomial Form):\n   Performing Z-transform on the above difference equation (assuming zero initial conditions) yields the system's pulse transfer function:\n   G(z) = Y(z)/U(z) = (b₀ + b₁z^{-1} + ... + b_mz^{-m}) / (1 + a₁z^{-1} + ... + a_nz^{-n})\n   Here, z^{-1} is the unit delay operator.\n3. Polynomial Representation:\n   Define two polynomials:\n   - Denominator polynomial (autoregressive part): A(z) = 1 + a₁z^{-1} + a₂z^{-2} + ... + a_nz^{-n}\n   - Numerator polynomial (moving average part): B(z) = b₀ + b₁z^{-1} + b₂z^{-2} + ... + b_mz^{-m}\n   Then the system can be concisely described as: A(z)Y(z) = B(z)U(z)\n   This polynomial representation of A(z) and B(z) is the basis for describing controlled objects in adaptive algorithms such as self-tuning control.\n   (Note: There is an ambiguity in the original answer: A(z) is the forward operator expression, and A(z^{-1}) is the backward operator expression, which should not be mixed; the same applies to B(z). Moreover, for input and output variables, Y(z) and U(z) (Z-transform forms) are usually not used directly when describing the time-domain relationship, and y(k) and u(k) (discrete-time domain forms) are more appropriate.)",
"Rate": 3,
"Category": "FDC",
"Comment": "The answer is not correct enough."
}


{
"Question15": "What is the meaning of the order of a polynomial?",
"Answer": "In system modeling, the order of a polynomial is a mathematical concept with core physical significance.\n1. Mathematical Definition:\n   The order (or degree) of a polynomial is the highest power of its terms. In the system transfer function polynomial A(z) = 1 + a₁z⁻¹ + ... + aₙz⁻ⁿ, the order is n.\n2. Physical Meanings:\n   - Complexity of system dynamics: The order n directly represents the minimum number of independent state variables required to describe the system dynamics.\n   - 'Memory' length of the system: It reflects how far back the system's current output is affected by the historical inputs and outputs. A higher order means a longer dynamic history of the system and more complex behavior.\n   - Number of energy storage elements: In many physical systems, the order corresponds to the number of independent energy-storing elements (such as capacitors, inductors, masses, and springs) in the system.\n3. Importance in Control:\n   - Stability analysis: The number of system poles is equal to the order of the denominator polynomial, and poles determine the system's stability and response speed.\n   - Controller design: The order of the controller (e.g., PID) needs to match the order of the controlled object to achieve effective control. In adaptive control, correctly setting the polynomial order of the model is a prerequisite for successful parameter estimation; an order that is too low (unmodeled dynamics) or too high will lead to performance degradation.\n4. Example:\n   A second-order system (e.g., mass-spring-damper) has two independent states (position and velocity). Its motion equation is a second-order differential equation, and the denominator polynomial of its transfer function is second-order. It may exhibit oscillatory behavior, which is impossible for a first-order system.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
},
{
"Question16": "What are the zeros and poles of an open-loop system, and what do they mean?",
"Answer": "In control systems, the zeros and poles of an open-loop system are core concepts for analyzing its inherent dynamic characteristics, usually derived from its transfer function.\n1. Definitions:\n   - Poles: The roots that make the denominator of the system transfer function zero. They determine the modes of the system's free motion (when there is no input).\n   - Zeros: The roots that make the numerator of the system transfer function zero. They determine the system's ability to block specific input frequencies.\n2. Physical Meanings:\n   Poles:\n   - Stability: The position of poles in the complex plane s (for continuous systems) or z (for discrete systems) directly determines the system's stability. For continuous systems, if all poles are in the left half-plane, the system is stable; if on the imaginary axis, it is critically stable; if in the right half-plane, it is unstable. For discrete systems, it depends on whether the poles are inside the unit circle.\n   - Dynamic response speed and mode: The absolute value of the real part of a pole (distance from the imaginary axis) determines the attenuation or divergence speed of that mode. Poles also determine whether the response is monotonic or oscillatory (complex poles).\n   Zeros:\n   - 'Shaping' effect on response: Zeros do not affect the system's stability itself, but they act like a 'filter' that can enhance or weaken the response of specific frequency components. A zero can attract nearby responses, which may cause overshoot.\n   - Blocking effect: If the Laplace transform of an input signal equals a zero of the system, the input cannot excite an output (theoretical blocking).\n3. Example:\n   A low-pass filter uses its poles to pass low-frequency signals, while its zeros (if any) can be used to quickly attenuate high-frequency signals.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
},
{
"Question17": "What are the zeros and poles of a closed-loop system, and what do they mean?",
"Answer": "The zeros and poles of a closed-loop system are mathematical representations of the dynamic characteristics of the entire closed-loop system after introducing feedback control. They are derived from the closed-loop transfer function.\n1. Physical Meanings and Importance:\n   Closed-loop poles:\n   - Core significance: The original answer has an error here. The position of closed-loop poles directly determines the final stability of the feedback control system, but the response speed and dynamic performance (such as overshoot and settling time) are determined by both closed-loop poles and closed-loop zeros together.\n   One of the core goals of controller design is to adjust the controller parameters to 'place' the closed-loop poles in a desired region of the complex plane that can provide excellent performance.\n   Closed-loop zeros:\n   - The original answer has ambiguities here. The statement 'controller zeros' is rarely found in academic literature; closed-loop zeros are usually determined by the zeros of the controlled object and the structure of the controller (not explicit 'controller zeros'). The phrase 'response shape of the closed-loop system' is also non-standard; a more accurate description is that closed-loop zeros affect the amplitude and phase characteristics of the system response.\n   The blocking effect of closed-loop zeros acts on the input of the entire closed-loop system.\n2. Relationship with Open-loop Zeros and Poles:\n   The controller changes the system's poles through feedback, thereby altering the system's dynamics. This is the core 'magic' of feedback.\n   Closed-loop zeros usually include the zeros of the open-loop object. However, a key point missing in the original answer is: The closed-loop system can allow stable pole-zero cancellation, but unstable pole-zero cancellation is not allowed (as it may hide potential instability).\n3. Example:\n   An open-loop object that is inherently unstable (with poles in the right half-plane) can be transformed into a stable system with excellent performance by designing an appropriate controller, which places all closed-loop poles in the left half-plane.",
"Rate": 3,
"Category": "FDC",
"Comment": "The answer is incorrect."
},

{
"Question18": "What is the meaning of the time delay of a controlled object?",
"Answer": "The time delay of a controlled object, also known as pure lag, refers to the time interval between when a control action is applied to the controlled object and when the action begins to produce an observable effect on the output of the controlled object.\n1. Mathematical Description:\n   In a continuous-time model, time delay is usually expressed as e^(-τs), where τ is the time delay duration and s is the Laplace variable.\n2. Physical Causes:\n   - Material/energy transfer: For example, when transporting fluids or heat in a long pipeline, after the control valve acts, it takes time for the fluid or heat to reach the measurement point.\n   - Signal processing/computation time: For example, it takes time for sensor data to be transmitted back to the controller, for the controller to perform CALs, and for the actuator to respond.\n   - Chemical/biological processes: For example, some chemical reactions or fermentation processes require a certain induction period.\n3. Impact on Control:\n   - Stability challenge: Time delay seriously degrades the phase margin of the control system, making the closed-loop system more prone to oscillation or even instability. To maintain stability, the controller gain often has to be reduced, which in turn sacrifices response speed and control performance.\n   - Performance limitation: Time delay fundamentally limits the optimal performance that any controller can achieve, as it introduces an insurmountable information lag.\n4. Example:\n   In the paper production process, the sensor that measures paper thickness is installed at a distance from the rolling mill. After adjusting the roll gap, it takes a period of time (time delay) for the paper with the new thickness to reach the sensor position.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}


{
"Question19": "What are the differences and connections between white noise and colored noise?",
"Answer": "White noise and colored noise are two models used to describe random interference signals.\n1. Definitions and Core Characteristics:\n   - White noise: An idealized random process whose power spectral density is a constant across all frequencies (similar to white light containing all colors). Its core feature is that there is no correlation between consecutive time instants (delta-function autocorrelation).\n   - Colored noise: A more common random process in practice, whose power spectral density is not uniform across different frequencies. Its core feature is that values at consecutive time instants are correlated.\n2. Differences:\n   - Correlation: White noise has no autocorrelation (values at different time instants are uncorrelated); colored noise has autocorrelation (values at consecutive time instants are correlated).\n   - Spectrum: White noise has a flat spectrum, with equal energy distributed across all frequencies; colored noise has a non-flat spectrum, with stronger energy concentrated at specific frequencies.\n   - Practicality: White noise is an ideal model that is difficult to realize physically; colored noise is more consistent with actual interference scenarios, such as ocean waves, temperature fluctuations, and electronic circuit noise.\n3. Connections:\n   - Colored noise can usually be regarded as the output of white noise passing through a dynamic system (shaping filter). For example, if white noise passes through a low-pass filter, the resulting signal is colored noise with stronger low-frequency energy.\n   - In system identification and filtering theory, unknown colored noise is often modeled as the output of a linear system driven by white noise. This modeling method simplifies the analysis process and the design of related algorithms.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}

{
"Question20": "What is the meaning of the Controlled AutoRegressive Integrated Moving Average (CARIMA) model?",
"Answer": "The Controlled AutoRegressive Integrated Moving Average (CARIMA) model is a highly important and practical discrete-time model in fields such as self-tuning control.\n1. Name Interpretation:\n   - Controlled: Indicates that the model includes an input variable u(k).\n   - AutoRegressive: Means the model output y(k) depends on its own past values, such as y(k-1), y(k-2), etc.\n   - Moving Average: Has two meanings here: first, it refers to the weighted average of past values of the input u(k); second, it refers to the weighted average of past values of the random noise e(k).\n2. Mathematical Model:\n   Its standard form is:\n   A(z⁻¹) y(k) = B(z⁻¹) u(k) + [C(z⁻¹) / Δ] e(k)\n   Where:\n   - A(z⁻¹), B(z⁻¹), and C(z⁻¹) are polynomials with respect to the backward shift operator z⁻¹.\n   - Δ = 1 - z⁻¹ is the difference operator, which enables the model to naturally eliminate step-type disturbances.\n   - e(k) is usually assumed to be a white noise sequence.\n3. Core Advantages and Applications:\n   - Built-in Integrator: Due to the noise term being divided by Δ, the CARIMA model can perfectly describe non-stationary processes where the output drifts under the influence of constant random disturbances. Many industrial processes (such as material balance) have this characteristic.\n   - Foundation for Self-Tuning Control: Advanced algorithms like the well-known Generalized Predictive Control (GPC) are developed based on the CARIMA model. This is because the model can naturally handle non-stationary disturbances, thereby achieving a control effect with no steady-state error.\n4. Example:\n   Consider the water level control of a water tank. Even if the opening degrees of the inlet valve and outlet valve are fixed, random fluctuations in water flow (colored noise) will cause the water level to drift slowly. The CARIMA model can well describe this characteristic of 'non-stationary changes in water level under random disturbances' and help design a controller that can accurately stabilize the water level at the set value.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}


{
"Question21": "In self-tuning control, why is it necessary to estimate model parameters?",
"Answer": "In self-tuning control, estimating model parameters is a core link for realizing its 'self-optimization' function, mainly for the following reasons:\n1. Addressing Uncertainties: The core application scenario of self-tuning control is when the mathematical model parameters of the controlled object are unknown or slowly time-varying. Parameter estimation uses online measured input-output data of the system to identify these unknown parameters in real time, providing a basis for controller design.\n2. Updating the Controller: Self-tuning control usually adopts the 'separation principle' and consists of two alternating steps: parameter estimation and controller design. First, the current model parameters of the system are obtained through estimation, and then the controller parameters are recalculated based on this updated model. Only in this way can the controller adapt to changes in system dynamics.\n3. Improving Performance and Robustness: Through continuous online parameter estimation, the self-tuning controller can track the drift of system characteristics (such as equipment aging, environmental changes) and adjust the control strategy accordingly, thereby maintaining optimal control performance and high robustness under changing operating conditions.\n4. Example: The reaction rate of a chemical reactor will slow down due to the attenuation of catalyst activity. Self-tuning control estimates the reaction rate parameters online and adjusts the parameters of the feed flow controller accordingly to ensure stable product quality.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
},
{
"Question22": "What is the meaning of process identification?",
"Answer": "Process identification is a science and technology that establishes a mathematical model between the input and output variables of a dynamic system through experimental data and mathematical methods.\n1. Core Idea: It converts a 'black-box' or 'grey-box' system into a mathematical expression (such as a transfer function or state-space equation) that can describe its dynamic behavior by applying test signals and observing its responses.\n2. Key Elements:\n   - Experimental Design: Select appropriate input signals (such as Pseudorandom Binary Sequence, PRBS) to fully excite all dynamic modes of the system.\n   - Model Structure Selection: Determine the type of model (such as linear/nonlinear, order, time delay).\n   - Parameter Estimation: Under the selected model structure, determine the specific parameter values to make the model output match the actual system output as closely as possible.\n   - Model Validation: Use another set of data not used for identification to verify the accuracy and applicability of the model.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
},
{
"Question23": "What is the meaning of parameter estimation?",
"Answer": "Parameter estimation is a specific step in process identification. Its meaning is: under the premise that the mathematical structure of the model (such as the form of a difference equation) is known, use the observed input-output data of the system to determine the numerical values of the unknown constant coefficients (i.e., parameters) in the model.\n1. Relationship Analysis: Process identification is a broader category that includes two major tasks: model structure determination and parameter estimation. It can be understood as: first, determine the 'skeleton' (structure) of the model through process identification, and then fill in the 'flesh and blood' (specific parameter values) through parameter estimation.\n2. Objective: Find a set of parameters such that the model based on these parameters can best fit the experimental data. The criterion for measuring 'goodness' is usually a loss function, such as the sum of squared errors in the least squares method.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
},
{
"Question24": "Please summarize the steps of process identification?",
"Answer": "Process identification usually follows a systematic process to ensure the reliability of the obtained model:\n1. Experimental Design: Design and execute an identification experiment. Select input signals that can fully excite all interested dynamic modes of the system, and collect input-output data.\n2. Data Preprocessing: Process the collected raw data, including removing trend terms, filtering and denoising, to improve data quality.\n3. Model Structure Selection: Based on prior knowledge or data analysis, select the type of candidate model (such as ARX, OE, state-space) and determine its order and time delay.\n4. Parameter Estimation: Use estimation methods (such as the least squares method) to determine the unknown parameters in the selected model structure based on the preprocessed data.\n5. Model Validation: This is a crucial step. Use another set of data that did not participate in parameter estimation to validate the model. Evaluation indicators include the fitting degree between the model output and the actual output, and the white noise characteristics of residuals (prediction errors). If the validation fails, return to Step 3 to reselect the model structure.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}


{
"Question25": "What is the meaning of 'Least Squares'?",
"Answer": "Least Squares is a fundamental and most commonly used parameter estimation method. Its core meaning is: to find a set of model parameters such that the sum of the squares of the differences between the model's predicted values and the actual observed values is minimized.\n1. Mathematical Expression:\n   For the model y(k) = φᵀ(k)θ, where φ(k) is the data vector and θ is the parameter vector to be estimated. The Least Squares estimate θ̂ minimizes the following performance index:\n   J(θ) = Σₖ₌₁ᴺ [y(k) - φᵀ(k)θ]²\n   (Note: Σₖ₌₁ᴺ denotes the sum from k=1 to k=N; φᵀ(k) denotes the transpose of the data vector φ(k); θ̂ denotes the estimated value of the parameter vector θ.)\n2. Intuitive Understanding:\n   It pursues 'the closest fit on the whole'. By minimizing the sum of squares, larger errors are given greater penalties, thereby obtaining the statistically optimal fit between the model and the actual data.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}

{
"Question26": "What is the basic principle of Recursive Least Squares (RLS)?",
"Answer": "Recursive Least Squares (RLS) is an efficient online implementation form of the Least Squares method. Its basic principle is: after obtaining new observation data, instead of reprocessing all historical data, it uses the previous parameter estimation result and the covariance matrix to perform a simple update through a recursive formula, thereby obtaining a new parameter estimation value.\n1. Core Formulas:\n   The recursive process includes three basic equations:\n   - Gain Update:\n     \( K(k) = \frac{P(k-1)\phi(k)}{\lambda + \phi^T(k)P(k-1)\phi(k)} \)\n   - Parameter Update:\n     \( \hat{\theta}(k) = \hat{\theta}(k-1) + K(k)[y(k) - \phi^T(k)\hat{\theta}(k-1)] \)\n   - Covariance Update:\n     \( P(k) = \frac{1}{\lambda}[I - K(k)\phi^T(k)]P(k-1) \)\n   Where, \( \lambda \) is the forgetting factor, used to track time-varying parameters.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}

{
"Question27": "What are the advantages of Recursive Least Squares (RLS) compared to Batch Least Squares?",
"Answer": "Recursive Least Squares (RLS) has several advantages over Batch Least Squares, which are detailed as follows:\n1. Computational Efficiency:\n   - RLS: High. It only performs a fixed number of operations in each sampling period, with constant computational load and memory usage.\n   - Batch Least Squares: Low. It needs to process the entire data matrix, and both computational load and memory requirements increase sharply when the data volume is large.\n2. Real-time Performance:\n   - RLS: Supports online and real-time estimation. It updates the estimated value immediately upon obtaining a new set of data.\n   - Batch Least Squares: Can only be processed offline. CALs can only be performed after all data has been collected.\n3. Application Scenarios:\n   - RLS: Suitable for online applications such as adaptive control, real-time monitoring, and parameter tracking.\n   - Batch Least Squares: Suitable for non-real-time scenarios such as offline analysis and experimental data processing.\n4. Tracking Time-Varying Parameters:\n   - RLS: Yes. By introducing a forgetting factor, it can weaken the influence of old data and track slowly changing parameters.\n   - Batch Least Squares: No. It treats all historical data equally and cannot track time-varying parameters.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}


{
"Question28": "What are the statistical properties of Least Squares parameter estimation?",
"Answer": "Under certain assumptions (mainly that the noise is white noise), Least Squares estimation has good statistical properties:\n1. Unbiasedness: If the noise is uncorrelated with the regression vector φ(k), the expected value of the parameter estimate is equal to the true value, i.e., E{θ̂} = θ₀.\n2. Consistency: As the number of samples approaches infinity, the parameter estimate converges to the true value with probability 1.\n3. Efficiency (Gauss-Markov Theorem): Among all linear unbiased estimates, the Least Squares estimate has the smallest variance. This means it is the 'optimal' linear unbiased estimate.\n4. Asymptotic Normality: When the sample size is large, the distribution of the parameter estimate approaches a normal distribution.\nNote: When the noise is colored noise, the ordinary Least Squares estimate loses its unbiasedness and consistency. In this case, improved methods such as the Extended Least Squares (ELS) or Instrumental Variable (IV) method need to be adopted.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
},

{
"Question29": "Please provide the M code for the simulation experiment of the recursive least squares identification algorithm written in MATLAB.",
"Answer": "The following code provides a flexible simulation framework that allows users to freely select the order and time delay of the controlled object. The core functions include generating the controlled object, input signals and noise, implementing the RLS identification algorithm, and verifying the identification effect through result display and visualization.\nMATLAB Code:\n```matlab\n% Recursive Least Squares Parameter Identification Simulation\nclear; clc; close all;\n% ========== User Setting Area ==========\nna = 2;     % Order of the denominator polynomial A(z^-1) of the controlled object\nnb = 1;     % Order of the numerator polynomial B(z^-1) of the controlled object\nnk = 2;     % Time delay of the controlled object (unit: sampling period)\nN = 1000;   % Total simulation steps\nlambda = 0.99; % Forgetting factor (0.95 < lambda <= 1)\n% =======================================\n\n% 1. Generate the real controlled object (ARX model: A(z)y(k) = B(z)u(k - nk) + e(k))\nA_true = [1, -0.8, 0.1];   % A(z) = 1 - 0.8z^-1 + 0.1z^-2 (when na=2)\nB_true = [0, 0, 0.5, 0.3]; % B(z) = 0.5z^{-2} + 0.3z^{-3} (when nb=1, nk=2. Note the leading zero padding)\n\n% Ensure the polynomial order matches the settings\nif length(A_true) ~= na+1, error('A_true length mismatch with na'); end\nif length(B_true) ~= nb+nk, error('B_true length mismatch with nb and nk'); end\n\n% 2. Generate input signal and noise\nu = randn(N, 1);        % Input: White noise sequence\ne = 0.1 * randn(N, 1);  % Measurement noise: White noise\n\n% 3. Simulate the output of the real system\ny = filter(B_true, A_true, u) + e;\n\n% 4. Initialize RLS parameters\ntheta_hat = zeros(na + nb, 1);      % Initial parameter estimation value [a1, a2, ..., b0, b1, ...]^T\nP = 1000 * eye(na + nb);            % Initial covariance matrix\ntheta_history = zeros(na + nb, N);  % Used to record the parameter estimation history\n\n% 5. Recursive Least Squares identification main loop\nfprintf('Starting Recursive Least Squares identification...\\n');\nfor k = max(na, nb+nk)+1 : N % Start from a sufficiently late time to have enough historical data\n    % Construct data vector φ(k) = [-y(k-1), ..., -y(k-na), u(k-nk), ., u(k-nk-nb+1)]^T\n    phi = [-y(k-1:-1:k-na); u(k-nk:-1:k-nk-nb+1)];\n\n    % Core RLS update formulas\n    K = (P * phi) / (lambda + phi' * P * phi);   % Calculate gain\n    y_hat = phi' * theta_hat;                    % Predicted output based on old parameters\n    epsilon = y(k) - y_hat;                      % Prediction error\n    theta_hat = theta_hat + K * epsilon;         % Update parameter estimation\n    P = (1/lambda) * (eye(na+nb) - K * phi') * P;% Update covariance matrix\n\n    % Record results\n    theta_history(:, k) = theta_hat;\nend\n\n% 6. Display the final identification results\nfprintf('\\n=== Identification Results ===\\n');\nfprintf('True parameters A: '); fprintf('%.4f ', A_true(2:end)); fprintf('\\n');\nfprintf('Estimated parameters A: '); fprintf('%.4f ', theta_hat(1:na)'); fprintf('\\n');\nfprintf('True parameters B: '); fprintf('%.4f ', B_true(nk+1:end)); fprintf('\\n');\nfprintf('Estimated parameters B: '); fprintf('%.4f ', theta_hat(na+1:end)'); fprintf('\\n');\n\n% 7. Plot results\nfigure;\nsubplot(2,1,1);\nplot(1:N, theta_history');\nxlabel('Sampling Steps'); \nylabel('Parameter Estimation Value');\ntitle('Parameter Estimation Convergence Process');\ngrid on;\nlegend(cellstr([strcat(repmat('a', na, 1), num2str((1:na)')), strcat(repmat('b', nb, 1), num2str((0:nb-1)'))]));\n\nsubplot(2,1,2);\nt_final = 200: N; % Plot the output comparison for the last period of time\ny_est = filter([0; theta_hat(na+1:end)], [1; theta_hat(1:na)], u);\nplot(t_final, y(t_final), 'b', t_final, y_est(t_final), 'r--');\nxlabel('Sampling Steps'); ylabel('System Output');\ntitle('Comparison between Real System Output and Model Predicted Output');\nlegend('Real Output', 'Model Prediction');\ngrid on;\n```",
"Rate": 1,
"Category": "PRG",
"Comment": "none"
}

{
"Question30": "Why is it necessary to study the 'Extended Least Squares (ELS)' method?",
"Answer": "The study of the Extended Least Squares (ELS) method is to solve the fundamental defect of ordinary Least Squares estimation being biased under colored noise.\n1. Motivation for ELS: When the system's noise term e(k) is not white noise but colored noise (i.e., correlated between consecutive time instants), the noise term will be correlated with the regression vector φ(k). This violates the basic assumption of ordinary Least Squares, resulting in the estimation result no longer being an unbiased and consistent estimate of the true parameters.\n2. Solution of ELS: The core idea of ELS is to include the noise model in the parameters to be estimated. It models the colored noise as the output of a linear system driven by white noise (usually an AutoRegressive, AR model), and then combines the parameters of this noise model with the original parameters of the system to form an 'extended' parameter vector for joint estimation.\n3. Advantages: Through this extension, the new regression vector is uncorrelated with the new noise (which is white noise at this time), so that the estimation result can regain unbiasedness and consistency. It provides a means to obtain an accurate model under more general noise conditions (when the noise is an ARMA process).\n4. Example: When measuring temperature, sensor noise may not be pure white noise but have temporal autocorrelation. Using ordinary Least Squares to identify the heating system model will produce bias, while using ELS can identify both the system dynamics and noise characteristics simultaneously, resulting in a more accurate model.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
},
{
"Question31": "Why is it necessary to study the 'Recursive Least Squares with Forgetting Factor' method?",
"Answer": "The core purpose of studying the Recursive Least Squares (RLS) with Forgetting Factor method is to enable the algorithm to track the parameter changes of time-varying systems online.\n1. Limitation of Ordinary RLS: Ordinary RLS treats all historical data equally. As the amount of data increases, the impact of new data on parameter updates becomes smaller and smaller. This causes the algorithm to 'forget' the knowledge learned in the early stage, but at the same time, it loses the ability to track parameter changes and eventually converges to a fixed value.\n2. Role of the Forgetting Factor: The forgetting factor (λ, usually 0.95 < λ < 1) systematically weakens the influence of old data by introducing a discount factor in the covariance update formula.\n   - Mathematical Representation: In the covariance update equation P(k) = (1/λ)[I - K(k)φᵀ(k)]P(k-1), λ < 1 prevents P(k) from approaching zero infinitely, thereby maintaining a certain gain K(k) and keeping the algorithm sensitive to new data at all times.\n   - Effect: The parameter estimate can adjust with the slow changes of the true system parameters, realizing adaptive identification of time-varying systems.\n3. Application Scenarios: Suitable for all occasions where system dynamics may drift slowly, such as industrial processes with catalyst activity attenuation, equipment wear, and changes in environmental conditions.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}

{
"Question32": "Please briefly describe the basic principle and basic structure of Self-Tuning Control (STC).",
"Answer": "Self-Tuning Control (STC) is an adaptive control strategy that combines online parameter estimation with controller parameter design. (Note: The original answer has an incomplete description here—STC actually includes both direct and indirect algorithms, not just direct adaptive algorithms.)\n1. Basic Principle:\n   Its core idea is 'identification while control'. It treats the controlled object as a model with unknown parameters. During the operation of the control system, it continuously uses real-time measured input-output data to estimate the model parameters of the controlled object online. Then, based on this continuously updated model, it redesigns (or calculates) the controller parameters, so that the performance of the closed-loop system always remains at the desired level. (Note: The original description only applies to indirect adaptive algorithms; direct STC algorithms adjust controller parameters directly without explicit model parameter estimation.)\n2. Basic Structure (Two Loops):\n   - Inner Loop (Control Loop): Composed of a conventional feedback controller and the controlled object, responsible for implementing basic feedback control. The parameters of this controller are adjustable.\n   - Outer Loop (Adaptive Loop): Consists of two functional modules:\n     1. Parameter Estimator: Uses algorithms such as Recursive Least Squares (RLS) to estimate the model parameters of the controlled object in real time online.\n     2. Controller Designer: Based on the latest parameter estimates and preset control objectives (such as minimum variance, pole placement), it solves online for new parameter values of the controller and updates the controller in the inner loop.",
"Rate": 2,
"Category": "FDC",
"Comment": "The answer is basically correct."
}

{
"Question33": "What are the differences and connections between Self-Tuning Regulator (STR) and Self-Tuning Control (STC)?",
"Answer": "Self-Tuning Regulator (STR) and Self-Tuning Control (STC) are closely related but distinct adaptive control technologies, with their differences and connections detailed as follows:\n1. Differences:\n   - Control Objective:\n     - STR: Focuses on regulation problems. Its main goal is to suppress disturbances and maintain the system output at a constant setpoint (usually the setpoint is 0).\n     - STC: Focuses on servo problems. Its main goal is to make the system output track a time-varying reference signal quickly and accurately.\n   - Setpoint/Reference Signal:\n     - STR: The setpoint is fixed and does not change over time.\n     - STC: The reference signal (equivalent to the 'setpoint' in STR) changes with time.\n   - Complexity:\n     - STR: Relatively simple. It is the basic form of self-tuning control technologies.\n     - STC: More complex. Its design must explicitly consider the dynamics of the time-varying reference signal to ensure effective tracking performance.\n2. Connections:\n   - STR is a special case of STC. It can be regarded as STC where the reference signal is constantly zero.\n   - Both share the core idea and basic structure of 'parameter estimation + controller design': they both rely on online parameter estimation to obtain the latest model of the controlled object, and then update the controller parameters based on the estimated model to adapt to system uncertainties.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}




{
"Question34": "What is the meaning of Minimum Variance?",
"Answer": "Minimum Variance is a statistical optimization index. Its meaning is: to find a control strategy that minimizes the variance (or mean square value) of the error between the system output y(k) and its expected value (usually the setpoint).\n1. Mathematical Expression: The control objective is to minimize the performance index \( J = E\left[ y(k+d) - y_r(k+d) \right]^2 \), where d is the system time delay and E represents the mathematical expectation.\n2. Intuitive Understanding: It pursues the smallest fluctuation of the system output around the setpoint under the influence of random disturbances. An ideal minimum variance controller can perfectly offset the impact of disturbances on the output, leaving only the minimum fluctuation caused by the system time delay and the irreducible part of the noise itself.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
},

{
"Question35": "What is the control objective of the Minimum Variance Regulator?",
"Answer": "The control objective of the Minimum Variance Regulator is: by calculating the appropriate control input u(k), minimize the variance of the output variable y(k) deviating from its setpoint (usually 0) when the system is affected by random disturbances.\nSpecifically, it aims to minimize the variance of the d-step ahead predicted output: \( J = E\left[ y(k+d|k) \right]^2 \), where \( y(k+d|k) \) is the optimal prediction of the output at time k+d made at time k.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}


{
"Question36": "When designing a minimum variance controller, why is it necessary to require the optimal prediction of the system output, and how to obtain the optimal prediction?",
"Answer": "When designing a minimum variance controller, requiring the optimal prediction of the system output and obtaining it are crucial:\nWhy is it necessary to require optimal prediction?\nBecause there is a time delay of dd steps in the controlled object, meaning that the control action u(k) added at the current time k will fully affect the output y(k+d) at time k+d. Therefore, to minimize the output variance at time k+d, we must predict the future output y(k+d|k) based on current and past information y(k), y(k-1),..., u(k), u(k-1),.... The control goal is to make this prediction as close as possible to the desired setpoint.\nHow to obtain the optimal prediction?\nFor the controlled autoregressive integrated moving average model, the optimal prediction can be obtained by solving the Diophantine equation. The steps are:\n a. Decompose the system model \( A(z^{-1})y(k) = B(z^{-1})u(k - d) + C(z^{-1})\xi(k) \).\n b. Solve the Diophantine equation: \( C(z^{-1}) = A(z^{-1})F(z^{-1}) + z^{-d}G(z^{-1}) \).\n c. Obtain the optimal d-step ahead predictor \( y^*(k + d|k) = \frac{G(z^{-1})}{C(z^{-1})}y(k) + \frac{B(z^{-1})F(z^{-1})}{C(z^{-1})}u(k) \).\nAmong them, \( F(z^{-1}) \) and \( G(z^{-1}) \) are polynomials obtained by solving the Diophantine equation.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}

{
"Question37": "What is the meaning of the polynomial Diophantine equation, and how to solve the polynomial Diophantine equation?",
"Answer": "The polynomial Diophantine equation (also known as Bezout's identity) is a key tool for polynomial decomposition in adaptive control. Its general form is \( A(z^{-1})X(z^{-1}) + B(z^{-1})Y(z^{-1}) = C(z^{-1}) \), where A, B, C are known polynomials, and X, Y are unknown polynomials. In minimum variance control, it specifically refers to the form \( C = AF + z^{-d}G \), whose purpose is to separate the known part and the future noise part in the prediction error.\nTo solve the polynomial Diophantine equation, it is essentially to solve a system of linear equations. The steps are:\n- Determine the degrees of polynomials: According to the degrees of A and d, determine the degrees of the unknown polynomials F and G. Usually, deg(F) = d - 1, deg(G) = deg(A) - 1.\n- Establish the system of equations: Expand the polynomial multiplication and equate the coefficients of the same powers of \( z^{-1} \) on both sides of the equation.\n- Solve the linear system of equations: This will result in a linear system of equations in the form \( Mx = c \), where x is a vector composed of all coefficients of F and G. By solving this system of equations, the coefficients of polynomials F and G can be obtained.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}

{
"Question38": "Please provide the M code for solving the polynomial Diophantine equation using MATLAB language.",
"Answer": "The following code implements a MATLAB function to solve the polynomial Diophantine equation in the form ( C(z^{-1}) = A(z^{-1})F(z^{-1}) + z^{-d}G(z^{-1}) ), a core tool in minimum variance control and self-tuning control. The function supports flexible input of polynomial coefficients and time delay, with detailed comments and example usage for easy implementation and verification.\nMATLAB Code:\n```matlab\n\nfunction [F, G] = diophantine(A, C, d)\n% Solve the Diophantine equation: C(z^{-1}) = A(z^{-1})F(z^{-1}) + z^{-d} G(z^{-1})\n% Inputs:\n%   A - Coefficient vector of polynomial A, e.g., [1, a1, a2]\n%   C - Coefficient vector of polynomial C\n%   d - Time delay\n% Outputs:\n%   F - Coefficient vector of polynomial F\n%   G - Coefficient vector of polynomial G\n% Determine the orders of polynomials F and G\ndeg_A = length(A) - 1;\ndeg_F = d - 1;\ndeg_G = deg_A - 1;\n% Initialize coefficient matrix M and vector c\nnum_coeff = deg_F + 1 + deg_G + 1; % Total number of coefficients in F and G\nM = zeros(num_coeff, num_coeff);\nc = zeros(num_coeff, 1);\n% Construct the linear system M * x = c\n% Structure of x: [f0, f1, ..., f_{deg_F}, g0, g1, ..., g_{deg_G}]'\n% 1. Process the equation C = A*F + z^{-d}*G\n% Iterate over each power of the resulting polynomial C\nfor k = 0 : (num_coeff - 1)  % Correct: Generate enough equations based on total number of coefficients\n    % Calculate the coefficient contributions of A*F and z^{-d}*G at the k-th power\n    for i = 0 : deg_F\n        if (k-i >= 0) && (k-i <= deg_A)\n            row = k + 1;\n            col = i + 1;\n            M(row, col) = A(k-i+1);\n        end\n    end\n    for j = 0 : deg_G\n        if (k - d - j == 0) % Note: The powers of z^{-d}*G start from d\n            row = k + 1;\n            col = (deg_F + 1) + j + 1;\n            M(row, col) = 1;\n        end\n    end\n    % Construct vector c\n    if k < length(C)\n        c(k+1) = C(k+1);\n    end\nend\n% 2. Solve the linear system\nx = M \\ c;\n% 3. Extract F and G from the solution vector x\nF = x(1 : deg_F+1)';\nG = x(deg_F+2 : end)';\nend\n% Example usage:\n% A = [1, -1.2, 0.5];\n% C = [1, 0.3, -0.1];\n% d = 2;\n% [F, G] = diophantine(A, C, d);\n```",
"Rate": 1,
"Category": "PRG",
"Comment": "none"
}
{
"Question39": "How to derive the equation of the minimum variance controller?",
"Answer": "The equation of the minimum variance controller is derived by setting the optimal prediction equal to the desired output (setpoint, usually 0).\n1. Write the optimal prediction equation:\n   [\r\n   y^*(k+d|k) = \frac{G(z^{-1})}{C(z^{-1})}y(k) + \frac{B(z^{-1})F(z^{-1})}{C(z^{-1})}u(k)\r\n   ]\r\n2. Set the control goal: Let the optimal prediction equal the setpoint ( y_r ) (for regulators, ( y_r = 0 )):\n   [\r\n   y^*(k+d|k) = y_r = 0\r\n   ]\r\n3. Derive the controller equation: Substitute and rearrange the above equation:\n   [\r\n   \frac{G(z^{-1})}{C(z^{-1})}y(k) + \frac{B(z^{-1})F(z^{-1})}{C(z^{-1})}u(k) = 0\r\n   ]\r\n   Multiply both sides by ( C(z^{-1}) ) to get the final minimum variance controller equation:\n   [\r\n   B(z^{-1})F(z^{-1})u(k) + G(z^{-1})y(k) = 0\r\n   ]\r\n   Or, rearranged to solve for ( u(k) ):\n   [\r\n   u(k) = -\frac{G(z^{-1})}{B(z^{-1})F(z^{-1})}y(k)\r\n   ]",
"Rate": 1,
"Category": "CAL",
"Comment": "none"
}

{
"Question40": "Please summarize the CAL steps of the minimum variance self-tuning regulator.",
"Answer": "The minimum variance self-tuning regulator follows the following steps in each sampling period:\n1. Data Collection: At time k, measure the new system output y(k) and read past input-output data such as y(k-1), ..., u(k-1), ...\n2. Parameter Estimation: Use online estimation algorithms like Recursive Least Squares to update the parameter estimates of polynomials A, B, and C in the system model (typically CARMA or CARIMA model \( \hat{A}(z^{-1})y(k) = \hat{B}(z^{-1})u(k-d) + \hat{C}(z^{-1})\xi(k) \)).\n3. Solve Diophantine Equation: Based on the currently estimated polynomials \( \hat{A} \) and \( \hat{C} \), and the known time delay d, solve the Diophantine equation \( \hat{C}(z^{-1}) = \hat{A}(z^{-1})\hat{F}(z^{-1}) + z^{-d}\hat{G}(z^{-1}) \) online to obtain polynomials \( \hat{F} \) and \( \hat{G} \).\n4. Calculate Control Quantity: Substitute \( \hat{B} \), \( \hat{F} \), and \( \hat{G} \) into the minimum variance control law \( \hat{B}(z^{-1})\hat{F}(z^{-1})u(k) + \hat{G}(z^{-1})y(k) = 0 \) to calculate and output the current control quantity u(k).\n5. Cycle: Increment k by 1, return to step 1, and repeat the entire process.",
"Rate": 1,
"Category": "CAL",
"Comment": "none"
}

{
"Question41": "Please summarize the CAL steps of the minimum variance self-tuning controller (note: only indirect algorithm is described, direct algorithm is not included).",
"Answer": "The minimum variance self-tuning controller (indirect algorithm) executes the following steps in each sampling period:\n1. Data Collection: Read the current system output \( y(k) \) and reference input \( r(k) \).\n2. Parameter Estimation: Use online recursive estimation algorithms (such as Recursive Least Squares) to update the parameter estimates \( \hat{A}, \hat{B}, \hat{C} \) of the system model \( A(z^{-1})y(k) = B(z^{-1})u(k-d) + C(z^{-1})\xi(k) \).\n3. Solve Diophantine Equation: Based on the latest \( \hat{A} \) and \( \hat{C} \), solve the equation \( \hat{C} = \hat{A}F + z^{-d}G \) to obtain polynomials \( F \) and \( G \).\n4. Calculate Control Quantity: Substitute \( \hat{B}, F, G \) into the minimum variance control law \( u(k) = \frac{G(z^{-1})}{\hat{B}(z^{-1})F(z^{-1})}(r(k+d) - y(k)) \) to calculate and apply the control quantity \( u(k) \).\n5. Cycle: Wait for the next sampling period and repeat steps 1-4.\n\nKey Point: This is an explicit algorithm because it first estimates the object model parameters and then designs the controller based on them. It should be noted that this summary only covers the indirect algorithm; the direct algorithm, which directly estimates controller parameters, is not included here.",
"Rate": 2,
"Category": "CAL",
"Comment": "The answer is basically correct; only the indirect algorithm is mentioned, and the direct algorithm is not included."
}
{
"Question42": "Why does the minimum variance self-tuning controller need to use the Extended Least Squares (ELS) method for parameter estimation?",
"Answer": "The minimum variance self-tuning controller needs to use the Extended Least Squares (ELS) method for parameter estimation mainly to address the issue of biased parameter estimation caused by noise correlation.\n1. Problem Origin:\n   - Theoretical Assumption: The minimum variance control theory assumes that the noise ξ(k) is white noise.\n   - Practical Situation: In actual systems, noise is often colored (i.e., correlated between consecutive time instants).\n   - Defect of Ordinary Least Squares: If the ordinary Recursive Least Squares (RLS) method is used, the colored noise term will be correlated with the past input-output data in the regression vector, violating the unbiased assumption of RLS and leading to biased and inconsistent parameter estimates.\n2. Solution of ELS:\n   - Parameter Expansion: Incorporate the parameters of the noise model \( C(z^{-1}) \) into the estimation vector.\n   - Regression Vector Construction: Add the a priori residual (as an estimate of the noise) to the regression vector, constructing a new regression vector that is uncorrelated with the noise.\n   - Effect: This enables the parameter estimation to regain unbiasedness and consistency, providing a more accurate model for controller design.\n3. Importance:\n   If the parameter estimation is biased, the controller designed based on it will fail to achieve true minimum variance control, resulting in degraded performance and possibly even system instability.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}
 
{
"Question43": "Please provide the MATLAB M code for the simulation experiments of the Minimum Variance Regulator, Minimum Variance Controller, Minimum Variance Self-Tuning Regulator, and Minimum Variance Self-Tuning Controller respectively.",
"Answer": "The following code implements four types of minimum variance control-related algorithms in MATLAB, including offline controllers (known model) and self-tuning controllers (unknown model, online parameter estimation). The code includes system simulation, control law calculation, parameter identification, and result visualization, with detailed comments for easy modification and verification.\nMATLAB Code:\n```matlab\n%% MATLAB Implementation of Minimum Variance Control Series Algorithms\nclear; clc; close all;\n\n%% Auxiliary Function: Solve Diophantine Equation (C = A*F + z^-d*G)\nfunction [F, G] = diophantine_solver(A, C, d)\n    deg_A = length(A) - 1;\n    deg_F = d - 1;\n    deg_G = deg_A - 1;\n    num_coeff = (deg_F + 1) + (deg_G + 1);\n    M = zeros(num_coeff, num_coeff);\n    c = zeros(num_coeff, 1);\n    \n    % Construct linear system M*x = c\n    for k = 0 : (num_coeff - 1)\n        % Contribution of A*F\n        for i = 0 : deg_F\n            if (k - i) >= 0 && (k - i) <= deg_A\n                M(k+1, i+1) = A(k - i + 1);\n            end\n        end\n        % Contribution of z^-d*G\n        for j = 0 : deg_G\n            if (k - d - j) == 0\n                M(k+1, (deg_F + 1) + j + 1) = 1;\n            end\n        end\n        % Right-hand side vector c\n        if k < length(C)\n            c(k+1) = C(k+1);\n        end\n    end\n    \n    x = M \\ c;\n    F = x(1 : deg_F + 1)';\n    G = x(deg_F + 2 : end)';\n    F(abs(F) < 1e-10) = 0;\n    G(abs(G) < 1e-10) = 0;\nend\n\n%% 1. Minimum Variance Regulator (MVR) - Known Model, Setpoint r=0\nfprintf('1. Running Minimum Variance Regulator...\\n');\n% System Parameters (CARMA Model: A(z^-1)y = B(z^-1)u + C(z^-1)xi)\nA = [1, -1.5, 0.7];    % A(z^-1) = 1 - 1.5z^-1 + 0.7z^-2\nB = [0, 1, 0.5];       % B(z^-1) = z^-1 + 0.5z^-2 (time delay d=1)\nC = [1, 0.2, -0.1];    % C(z^-1) = 1 + 0.2z^-1 - 0.1z^-2\nd = 1;                 % Time delay\n\n% Simulation Parameters\nN = 200;                % Total steps\ny_mvr = zeros(N, 1);    % System output\nu_mvr = zeros(N, 1);    % Control input\nxi = 0.1 * randn(N, 1); % White noise disturbance\n\n% Offline solve Diophantine equation\n[F, G] = diophantine_solver(A, C, d);\nfprintf('   Diophantine Equation Solution: F = [%.3f, %.3f], G = [%.3f, %.3f]\\n', F, G);\n\n% MVR Simulation Loop\nfor k = 3 : N\n    % Calculate system output (CARMA model)\n    y_mvr(k) = A(2:end) * [y_mvr(k-1); y_mvr(k-2)] + ...\n               B(2:end) * [u_mvr(k-d); u_mvr(k-d-1)] + ...\n               C * [xi(k); xi(k-1); xi(k-2)];\n    \n    % Minimum Variance Control Law: B(z^-1)F(z^-1)u(k) = -G(z^-1)y(k)\n    if k > d\n        y_vec = [y_mvr(k); y_mvr(k-1)];\n        BF = conv(B, F);\n        u_hist_len = length(BF) - 2;\n        u_hist = u_mvr(max(1, k-1) : -1 : max(1, k - u_hist_len));\n        \n        % Compute control input\n        if length(u_hist) < u_hist_len\n            u_hist = [u_hist; zeros(u_hist_len - length(u_hist), 1)];\n        end\n        u_mvr(k) = (-G * y_vec - BF(3:end) * u_hist) / BF(2);\n    end\nend\n\n%% 2. Minimum Variance Controller (MVC) - Known Model, Track Reference Signal\nfprintf('2. Running Minimum Variance Controller...\\n');\n% Reference Signal (step changes)\nr = [zeros(50, 1); 0.5*ones(50, 1); ones(50, 1); 0.5*ones(50, 1)];\n\n% Initialize variables\ny_mvc = zeros(N, 1);\nu_mvc = zeros(N, 1);\n\n% Use the same Diophantine solution as MVR\nfor k = 3 : N\n    % Calculate system output\n    y_mvc(k) = A(2:end) * [y_mvc(k-1); y_mvc(k-2)] + ...\n               B(2:end) * [u_mvc(k-d); u_mvc(k-d-1)] + ...\n               C * [xi(k); xi(k-1); xi(k-2)];\n    \n    % Control Law: B(z^-1)F(z^-1)u(k) = G(z^-1)(r(k+d) - y(k))\n    if k > d && (k + d) <= N\n        y_vec = [y_mvc(k); y_mvc(k-1)];\n        r_future = [r(k+d); r(k+d-1)];\n        BF = conv(B, F);\n        u_hist_len = length(BF) - 2;\n        u_hist = u_mvc(max(1, k-1) : -1 : max(1, k - u_hist_len));\n        \n        if length(u_hist) < u_hist_len\n            u_hist = [u_hist; zeros(u_hist_len - length(u_hist), 1)];\n        end\n        u_mvc(k) = (G * (r_future - y_vec) - BF(3:end) * u_hist) / BF(2);\n    end\nend\n\n%% 3. Minimum Variance Self-Tuning Regulator (MVSTR) - Unknown Model\nfprintf('3. Running Minimum Variance Self-Tuning Regulator...\\n');\n% Parameter Estimation Initialization (Extended Least Squares)\nna = length(A) - 1;  % Order of A\nnb = length(B) - 1 - d;  % Order of B\nnc = length(C) - 1;  % Order of C\ntheta_hat = 0.1 * randn(na + nb + 1 + nc, 1);  % [A; B; C] parameters\nP = 1000 * eye(na + nb + 1 + nc);  % Covariance matrix\nlambda = 0.99;  % Forgetting factor\n\n% Initialize variables\ny_mvstr = zeros(N, 1);\nu_mvstr = zeros(N, 1);\nepsilon = zeros(N, 1);  % A priori residual\n\n% MVSTR Simulation Loop\nfor k = max([na, nb + d, nc]) + 1 : N\n    % Calculate system output (true model)\n    y_mvstr(k) = A(2:end) * [y_mvstr(k-1); y_mvstr(k-2)] + ...\n                 B(2:end) * [u_mvstr(k-d); u_mvstr(k-d-1)] + ...\n                 C * [xi(k); xi(k-1); xi(k-2)];\n    \n    % Construct regression vector (Extended Least Squares)\n    phi = [-y_mvstr(k-1); -y_mvstr(k-2); ...\n           u_mvstr(k-d); u_mvstr(k-d-1); ...\n           epsilon(k-1); epsilon(k-2)];\n    \n    % Recursive Least Squares Update\n    K = P * phi / (lambda + phi' * P * phi);\n    epsilon_prior = y_mvstr(k) - phi' * theta_hat;\n    theta_hat = theta_hat + K * epsilon_prior;\n    P = (1 / lambda) * (P - K * phi' * P);\n    epsilon(k) = epsilon_prior;\n    \n    % Extract estimated parameters\n    A_hat = [1, theta_hat(1:na)'];\n    B_hat = [zeros(1, d), theta_hat(na+1:na+nb+1)'];\n    C_hat = [1, theta_hat(na+nb+2:end)'];\n    \n    % Online solve Diophantine equation\n    [F_hat, G_hat] = diophantine_solver(A_hat, C_hat, d);\n    \n    % Self-Tuning Control Law\n    if k > d\n        y_vec = [y_mvstr(k); y_mvstr(k-1)];\n        BF_hat = conv(B_hat, F_hat);\n        u_hist_len = length(BF_hat) - 2;\n        u_hist = u_mvstr(max(1, k-1) : -1 : max(1, k - u_hist_len));\n        \n        if length(u_hist) < u_hist_len\n            u_hist = [u_hist; zeros(u_hist_len - length(u_hist), 1)];\n        end\n        if abs(BF_hat(2)) > 1e-6  % Avoid division by zero\n            u_mvstr(k) = (-G_hat * y_vec - BF_hat(3:end) * u_hist) / BF_hat(2);\n        end\n    end\nend\n\n%% 4. Minimum Variance Self-Tuning Controller (MVSTC) - Unknown Model\nfprintf('4. Running Minimum Variance Self-Tuning Controller...\\n');\n% Re-initialize parameter estimation\ntheta_hat2 = 0.1 * randn(na + nb + 1 + nc, 1);\nP2 = 1000 * eye(na + nb + 1 + nc);\nepsilon2 = zeros(N, 1);\n\n% Initialize variables\ny_mvstc = zeros(N, 1);\nu_mvstc = zeros(N, 1);\n\n% MVSTC Simulation Loop\nfor k = max([na, nb + d, nc]) + 1 : N\n    % Calculate system output (true model)\n    y_mvstc(k) = A(2:end) * [y_mvstc(k-1); y_mvstc(k-2)] + ...\n                 B(2:end) * [u_mvstc(k-d); u_mvstc(k-d-1)] + ...\n                 C * [xi(k); xi(k-1); xi(k-2)];\n    \n    % Parameter Estimation (Extended Least Squares)\n    phi2 = [-y_mvstc(k-1); -y_mvstc(k-2); ...\n            u_mvstc(k-d); u_mvstc(k-d-1); ...\n            epsilon2(k-1); epsilon2(k-2)];\n    \n    K2 = P2 * phi2 / (lambda + phi2' * P2 * phi2);\n    epsilon_prior2 = y_mvstc(k) - phi2' * theta_hat2;\n    theta_hat2 = theta_hat2 + K2 * epsilon_prior2;\n    P2 = (1 / lambda) * (P2 - K2 * phi2' * P2);\n    epsilon2(k) = epsilon_prior2;\n    \n    % Extract estimated parameters\n    A_hat2 = [1, theta_hat2(1:na)'];\n    B_hat2 = [zeros(1, d), theta_hat2(na+1:na+nb+1)'];\n    C_hat2 = [1, theta_hat2(na+nb+2:end)'];\n    \n    % Online solve Diophantine equation\n    [F_hat2, G_hat2] = diophantine_solver(A_hat2, C_hat2, d);\n    \n    % Self-Tuning Control Law (track reference signal)\n    if k > d && (k + d) <= N\n        y_vec = [y_mvstc(k); y_mvstc(k-1)];\n        r_future = [r(k+d); r(k+d-1)];\n        BF_hat2 = conv(B_hat2, F_hat2);\n        u_hist_len = length(BF_hat2) - 2;\n        u_hist = u_mvstc(max(1, k-1) : -1 : max(1, k - u_hist_len));\n        \n        if length(u_hist) < u_hist_len\n            u_hist = [u_hist; zeros(u_hist_len - length(u_hist), 1)];\n        end\n        if abs(BF_hat2(2)) > 1e-6\n            u_mvstc(k) = (G_hat2 * (r_future - y_vec) - BF_hat2(3:end) * u_hist) / BF_hat2(2);\n        end\n    end\nend\n\n%% Result Visualization\n% Figure 1: Offline Controllers (MVR & MVC)\nfigure('Position', [100, 100, 1200, 800]);\n\nsubplot(2, 2, 1);\nplot(1:N, y_mvr, 'b-', 'LineWidth', 1.5);\ntitle('Minimum Variance Regulator (Setpoint r=0)');\nylabel('System Output y(k)');\nxlabel('Sampling Time k');\ngrid on;\n\nsubplot(2, 2, 2);\nplot(1:N, u_mvr, 'r-', 'LineWidth', 1.5);\ntitle('Control Input of MVR');\nylabel('Control Signal u(k)');\nxlabel('Sampling Time k');\ngrid on;\n\nsubplot(2, 2, 3);\nplot(1:N, y_mvc, 'b-', 1:N, r, 'g--', 'LineWidth', 1.5);\ntitle('Minimum Variance Controller (Track Reference)');\nylabel('System Output y(k)');\nxlabel('Sampling Time k');\nlegend('Output', 'Reference Signal', 'Location', 'best');\ngrid on;\n\nsubplot(2, 2, 4);\nplot(1:N, u_mvc, 'r-', 'LineWidth', 1.5);\ntitle('Control Input of MVC');\nylabel('Control Signal u(k)');\nxlabel('Sampling Time k');\ngrid on;\n\n% Figure 2: Self-Tuning Controllers (MVSTR & MVSTC)\nfigure('Position', [200, 200, 1200, 800]);\n\nsubplot(2, 2, 1);\nplot(1:N, y_mvstr, 'b-', 'LineWidth', 1.5);\ntitle('Minimum Variance Self-Tuning Regulator');\nylabel('System Output y(k)');\nxlabel('Sampling Time k');\ngrid on;\n\nsubplot(2, 2, 2);\nplot(1:N, u_mvstr, 'r-', 'LineWidth', 1.5);\ntitle('Control Input of MVSTR');\nylabel('Control Signal u(k)');\nxlabel('Sampling Time k');\ngrid on;\n\nsubplot(2, 2, 3);\nplot(1:N, y_mvstc, 'b-', 1:N, r, 'g--', 'LineWidth', 1.5);\ntitle('Minimum Variance Self-Tuning Controller');\nylabel('System Output y(k)');\nxlabel('Sampling Time k');\nlegend('Output', 'Reference Signal', 'Location', 'best');\ngrid on;\n\nsubplot(2, 2, 4);\nplot(1:N, u_mvstc, 'r-', 'LineWidth', 1.5);\ntitle('Control Input of MVSTC');\nylabel('Control Signal u(k)');\nxlabel('Sampling Time k');\ngrid on;\n\nfprintf('Simulation Completed!\\n');\n```",
"Rate": 1,
"Category": "PRG",
"Comment": "none"
}
{
"Question44": "What are the advantages and disadvantages of the Generalized Minimum Variance (GMV) self-tuning controller compared to the Minimum Variance self-tuning controller?",
"Answer": "The Generalized Minimum Variance (GMV) self-tuning controller has the following advantages and disadvantages compared to the Minimum Variance self-tuning controller:\n1. Applicability:\n   - Advantage: Can be applied to non-minimum phase systems, which is its biggest advantage.\n   - Disadvantage: More complex to design, requiring the selection of control weighting polynomials λ or Q.\n2. Control Performance:\n   - Advantage: Can trade off between output variance and control energy through weighting factors, avoiding excessive or drastic changes in control actions.\n   - Disadvantage: If the weighting factors are not selected properly, it may lead to conservative control performance and slower response.\n3. Robustness:\n   - Advantage: Usually has better robustness as it does not cancel non-minimum phase zeros.\n   - Disadvantage: Its output variance is not the theoretical absolute minimum compared to Minimum Variance control.\n4. Summary:\n   The GMV self-tuning controller sacrifices the theoretical minimum output variance in exchange for applicability to non-minimum phase systems, smooth control actions, and better robustness, making it a more practical algorithm in engineering.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}
{
"Question45": "Why can the generalized minimum variance self-tuning controller be applied to non-minimum phase systems?",
"Answer": "The generalized minimum variance self-tuning controller can be applied to non-minimum phase systems by modifying the performance index to avoid canceling the \( B(z^{-1}) \) polynomial of the system model. Here are the details: \n- **Strategy of Minimum Variance Control**: The minimum variance control law includes \( B(z^{-1}) \) in the denominator. If \( B(z^{-1}) \) has roots outside the unit circle (i.e., non-minimum phase zeros), the controller will try to cancel these unstable zeros to minimize the output variance, which leads to divergent control signals and system instability. \n- **Solution of Generalized Minimum Variance**: It introduces a weighted penalty on the control quantity (\( \lambda u^2(k) \)) into the performance index. This makes the controller consider the control energy while minimizing the output variance. As a result, the denominator of the derived control law becomes \( B(z^{-1}) + \lambda A(z^{-1}) \) instead of just \( B(z^{-1}) \). By properly selecting the weighting factor \( \lambda \), the new denominator polynomial can be ensured to be stable, even if the original \( B(z^{-1}) \) is unstable.",
"Rate": 1,
"Category": "CAL",
"Comment": "none"
}

{
"Question46": "How is the performance index of the generalized minimum variance controller defined?",
"Answer": "The performance index of the generalized minimum variance controller is an extension of the basic minimum variance index, and its general form is \( J = E\{[P(z^{-1})y(k+d) - R(z^{-1})r(k)]^2 + [Q(z^{-1})u(k)]^2\} \). Here are the details: \n- \( P(z^{-1}) \): Output weighting polynomial, used to filter the output signal and emphasize the performance of specific frequency bands. \n- \( R(z^{-1}) \): Reference input weighting polynomial, used to filter the reference signal. \n- \( Q(z^{-1}) \): Control input weighting polynomial, the most critical part, used to penalize the size or rate of change of the control quantity. The simplest form is \( Q(z^{-1}) = \lambda \) (a constant), i.e., directly penalizing the square of the control quantity. \n- Simplified form: In many applications, taking \( P=1 \) and \( R=1 \), the performance index is simplified to \( J = E\{[y(k+d) - r(k)]^2 + [Q(z^{-1})u(k)]^2\} \). When \( Q(z^{-1}) = \lambda \), it is the most common case with constant control weighting.",
"Rate": 1,
"Category": "CAL",
"Comment": "none"
}

{
"Question47": "How to select the weighting polynomials in the performance index of the generalized minimum variance controller?",
"Answer": "The selection of weighting polynomials in the performance index of the generalized minimum variance controller is a trade-off process, relying on engineering experience and specific control objectives, and it should ensure closed-loop system stability and eliminate steady-state errors. The details are as follows: \n- **Control Weighting \( Q(z^{-1}) \) (or \( \lambda \))**: \n  - Purpose: To balance output performance and control effort. \n  - Selection Method: Start from a small value (e.g., 0) and gradually increase until the control action is smooth and acceptable, while observing whether the output performance is within the allowable range. If the rate of change of the control quantity is too large, \( Q \) can be designed as a high-pass filter (e.g., \( Q = \lambda(1 - z^{-1}) \)) to penalize the change of the control quantity. \n  - Influence: The larger \( \lambda \) is, the weaker the control action, the slower the system response, and the better the robustness may be; the smaller \( \lambda \) is, the stronger the control action, the faster the response, but it may excite unmodeled dynamics and lead to instability. \n- **Output Weighting \( P(z^{-1}) \)**: \n  - Purpose: To shape the frequency response characteristics of the closed-loop system. \n  - Selection Method: Usually \( P = 1 \). If it is necessary to suppress interference at specific frequencies, \( P \) can be designed as a band-stop filter. If low-frequency tracking is required, it can be designed as a high-pass filter. \n- **Reference Weighting \( R(z^{-1}) \)**: \n  - Purpose: To preprocess the reference signal, for example, to make it change smoothly and reduce the impact on the system. \n  - Selection Method: Usually \( R = 1 \). It can be designed as a low-pass filter to smooth the reference command.",
"Rate": 2,
"Category": "CAL",
"Comment": "The answer is incomplete; it should include ensuring the stability of the closed-loop system and eliminating steady-state errors."
}
{
"Question48": "What is the difference between the Generalized Minimum Variance (GMV) controller and the Generalized Minimum Variance Self-Tuning Controller (GMV STC)?",
"Answer": "The Generalized Minimum Variance (GMV) controller and the Generalized Minimum Variance Self-Tuning Controller (GMV STC) differ in the following key aspects:\n1. Precondition:\n   - GMV Controller: The accurate mathematical model of the controlled object (polynomials A, B, C and time delay d) is completely known.\n   - GMV STC: The model parameters of the controlled object are unknown or slowly time-varying.\n2. Implementation Method:\n   - GMV Controller: Offline design. All CALs (such as solving the Diophantine equation) can be completed in advance, and the controller parameters are fixed during operation.\n   - GMV STC: Online real-time CAL. It includes an online parameter estimator that continuously updates the model parameters and adjusts the controller parameters in real time based on these updated parameters.\n3. Application Scenario:\n   - GMV Controller: Suitable for systems with fixed and known models.\n   - GMV STC: Suitable for systems with unknown models or time-varying parameters, and has adaptive capabilities.\n4. Complexity:\n   - GMV Controller: Relatively simple, as it does not require online parameter estimation or real-time parameter adjustment.\n   - GMV STC: More complex, as it needs to run parameter estimation algorithms and controller design algorithms online simultaneously.\n\nCore Connection: The GMV STC is a product of combining the design principle of the GMV controller with the self-tuning mechanism (online parameter estimation).",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}
{
"Question49": "What are the meanings of the implicit algorithm and explicit algorithm for self-tuning control?",
"Answer": "The implicit algorithm and explicit algorithm are two different strategies for implementing self-tuning control, with the difference lying in what parameters are directly estimated:\n1. Explicit Algorithm:\n   - Meaning: Directly estimate the model parameters of the controlled object online (e.g., coefficients of polynomials A, B, C). Then, based on these estimated model parameters, calculate the controller parameters by solving controller design equations (such as Diophantine equations) online.\n   - Process: Input-output data → Estimate object parameters → Design controller → Calculate control signal\n   - Advantage: Clear concept; directly obtains the object model, which is convenient for monitoring.\n   - Disadvantage: Large online computation load, requiring real-time solution of Diophantine equations, etc.\n2. Implicit Algorithm:\n   - Meaning: Establish a linear model for controller parameters through reparameterization. Then, directly estimate these controller parameters online, thereby omitting the controller design step.\n   - Process: Input-output data → Directly estimate controller parameters → Calculate control signal\n   - Advantage: Small computation load, simple structure, and no need to solve Diophantine equations online.\n   - Disadvantage: Cannot directly obtain the object model; the analysis of the convergence and stability of controller parameters is more complex.",
"Rate": 1,
"Category": "FDC",
"Comment": "none"
}

{
"Question50": "How to establish the controller parameter identification equation for realizing Generalized Minimum Variance Self-Tuning Control?",
"Answer": "Establishing the controller parameter identification equation is the key to realizing implicit Generalized Minimum Variance Self-Tuning Control. The core idea is to re-parameterize the system model to directly include the controller parameters to be solved. The derivation steps are as follows:\n1. Write the Generalized Minimum Variance Control Law:\nBy optimizing the performance index \( J = E\left\{ \left[ Py(k+d) - Rr(k) \right]^2 + \left[ Qu(k) \right]^2 \right\} \), the control law can be derived as: \( (B^*F + q_0A)u(k) = CRr(k) - Gy(k) \), where \( q_0 \) is the leading coefficient of \( Q \), \( B^* = z^dB \), and \( C = AF + z^{-d}G \).\n2. Re-parameterization:\nRewrite the above control law as: \( CRr(k) - Gy(k) - (B^*F + q_0A)u(k) = 0 \). Substitute the system model \( Ay(k) = Bu(k-d) + C\xi(k) \) and organize it. Finally, an equation of the form \( Ay(k) = \phi^T(k-d)\theta + C\xi(k) \) can be obtained, where \( \phi(k-d) \) is a data vector composed of past \( y, u, r \), and \( \theta \) is a vector composed of the coefficients of polynomials \( F, G, H \) (where \( H = B^*F + q_0A \)), which is the controller parameter vector to be estimated.\n3. Obtain the Identification Equation:\nThe final identification model is: \( y(k) = \phi^T(k-d)\theta + \xi(k) \) (here, it is assumed that the leading coefficients of \( A \) and \( C \) are 1, or corresponding processing has been done). This equation is linear with respect to the controller parameter \( \theta \), so the Recursive Least Squares method can be directly used for online estimation. In this way, the explicit step of first estimating \( A, B, C \) and then designing the controller is bypassed, and direct identification of controller parameters is realized.",
"Rate": 1,
"Category": "CAL",
"Comment": "none"
}

